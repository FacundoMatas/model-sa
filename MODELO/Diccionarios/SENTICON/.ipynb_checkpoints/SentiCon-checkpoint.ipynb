{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.metrics import *\n",
    "from sklearn import metrics\n",
    "from os import scandir, getcwd\n",
    "import json\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos donde esta el diccionario y los tweets, ademas de que experimento realizar. La variable \"clasificacion\" indica si se usa clasificación binaria o ternaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = \"senticon.es.xml\"\n",
    "tweets = 'tweets_preprocessed.xlsx'\n",
    "path = files_path=getcwd()\n",
    "file_1 = \"SentiCon_1\"\n",
    "file_2 = \"SentiCon_2\"\n",
    "directory = '\\\\TERNARIO\\\\'\n",
    "# directory = '\\\\BINARIO\\\\'\n",
    "\n",
    "\n",
    "filename_1 = path+directory+file_1\n",
    "filename_2 = path+directory+file_2\n",
    "nombre = 'SentiCon'\n",
    "\n",
    "clasificacion = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et \n",
    "parsedXML = et.parse( \"senticon.es.xml\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Los archivos xml se leen utilizando un parser Xml. Este diccionario esta en xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcols = ['Sentimiento', 'Palabra', 'Categoría', 'Posicion']\n",
    "dict_label = pd.DataFrame(columns=dfcols)\n",
    "\n",
    "for layer in parsedXML.iter('layer'):\n",
    "    for positive in layer.iter('positive'):\n",
    "        for lemma in positive.iter('lemma'):\n",
    "            word = lemma.text\n",
    "            sentiment = 'pos'\n",
    "            polarity = lemma.attrib.get('pol')\n",
    "            position = lemma.attrib.get('pos')\n",
    "           \n",
    "            \n",
    "            dict_label = dict_label.append(\n",
    "            pd.Series([sentiment, word, float(polarity),position],index=dfcols),ignore_index=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for positive in layer.iter('negative'):\n",
    "        for lemma in positive.iter('lemma'):\n",
    "            word = lemma.text\n",
    "            sentiment = 'neg'\n",
    "            polarity = lemma.attrib.get('pol')\n",
    "            position = lemma.attrib.get('pos')\n",
    "           \n",
    "            \n",
    "            dict_label = dict_label.append(\n",
    "            pd.Series([sentiment, word, float(polarity),position],index=dfcols),ignore_index=True)\n",
    "            \n",
    "            \n",
    "dict_label\n",
    "\n",
    "\n",
    "dict_label.to_excel('senticon.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como se eliminaron los acentos del conjunto de tweets, tambien se eliminan de las palabras del diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento_dict(row):\n",
    "    a,b = 'áéíóúü','aeiouu'\n",
    "    trans = str.maketrans(a,b)\n",
    "    replaced_links = row[\"Palabra\"].translate(trans)  \n",
    "    replaced_links = row['Palabra'].strip()\n",
    "    \n",
    "    return replaced_links   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicamos el preprocesamiento al diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label[\"Palabra\"] = dict_label.apply(preprocesamiento_dict, axis=1)\n",
    "dict_label = dict_label.drop_duplicates(subset=['Palabra'])\n",
    "\n",
    "dict_label = dict_label.loc[~dict_label['Palabra'].str.contains('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df[df.sentiment < clasificacion]\n",
    "df_clean = df_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label_stem = dict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividimos en conjuntos de prueba y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_clean.sample(frac=0.8,random_state=200)\n",
    "test=df_clean.drop(train.index)\n",
    "\n",
    "\n",
    "train = train[train.sentiment < clasificacion]\n",
    "\n",
    "\n",
    "df_clean = train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = dict_label.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = dict_label.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esta función calcula la polaridad promedio de una palabra que no se encuentra en el diccionario, a partir de las polaridades de los tweets donde aparece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polaridadPromedio(palabra):\n",
    "    tweetsQueTienenPalabra = df_clean[df_clean['text'].str.contains(palabra)]\n",
    "    #print(tweetsQueTienenPalabra)\n",
    "    avg = tweetsQueTienenPalabra[\"sentiment\"].mean()\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df_clean\n",
    "\n",
    "for id, tweet in df_aux.iterrows():\n",
    "    filtered_words = [word for word in tweet.text.split() if word not in stopwords.words('spanish')]\n",
    "    for word in filtered_words:\n",
    "        if dict_label.loc[dict_label['Palabra'].isin([word])]['Palabra'].any():\n",
    "            a=1\n",
    "        else:        \n",
    "            avg = polaridadPromedio(str(word))\n",
    "            df2 = pd.DataFrame([[word,avg]], columns=['Palabra','Categoría'])\n",
    "            dict_label = dict_label.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_sentimiento(text):\n",
    "    valor = 0\n",
    "    filtered_words = [word for word in str(text).split() if word not in stopwords.words('spanish')]\n",
    "    for word in filtered_words:\n",
    "        if dict_label.loc[dict_label['Palabra'].isin([word])]['Categoría'].any():           \n",
    "            try:                \n",
    "                valor = valor + float(dict_label.loc[dict_label['Palabra'].isin([word])]['Categoría'])\n",
    "            except:\n",
    "                print(\"error\")                \n",
    "    \n",
    "    return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esta función determina la polaridad de un tweet en funcion del diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_sentimiento(text):\n",
    "    tienePalabra = False\n",
    "    valor = 999999\n",
    "    filtered_words = [word for word in text.split() if word not in stopwords.words('spanish')]\n",
    "    for word in filtered_words:\n",
    "        if dict_label.loc[dict_label['Palabra'].isin([word])]['Categoría'].any(): \n",
    "            if not tienePalabra:\n",
    "                tienePalabra = True\n",
    "                valor = 0\n",
    "            try:                \n",
    "                valor = valor + float(dict_label.loc[dict_label['Palabra'].isin([word])]['Categoría'])\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(dict_label.loc[dict_label['Palabra'].isin([word])]['Categoría'])               \n",
    "    \n",
    "    return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se calcula la polaridad del conjunto de tweets de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correcto = 0\n",
    "max = 0\n",
    "min = 0\n",
    "ref = []\n",
    "testVal= []\n",
    "sentimentList = []\n",
    "\n",
    "if(clasificacion == 2):\n",
    "    #BINARIO\n",
    "    for id, tweet in test.iterrows(): \n",
    "        sentimiento = analisis_sentimiento(str(tweet.text))\n",
    "        sentimentList.append(sentimiento)\n",
    "        if(sentimiento != 999999):\n",
    "            if(sentimiento > max):\n",
    "                max = sentimiento\n",
    "            if(sentimiento < min):\n",
    "                min = sentimiento  \n",
    "            ref.append(1 if int(tweet.sentiment) == 1 else -1)      \n",
    "            if(sentimiento <= 3): \n",
    "                testVal.append(-1)\n",
    "            else:\n",
    "                testVal.append(1)\n",
    "        \n",
    "else:\n",
    "    #TERNARIO\n",
    "    for id, tweet in test.iterrows():    \n",
    "        sentimiento = analisis_sentimiento(str(tweet.text))        \n",
    "        if (int(tweet.sentiment) == 0):\n",
    "            valor_tweet = -1\n",
    "        else: \n",
    "            valor_tweet = int(tweet.sentiment)\n",
    "        if(sentimiento != 999999):\n",
    "            if(sentimiento > max):\n",
    "                max = sentimiento\n",
    "            if(sentimiento < min):\n",
    "                min = sentimiento\n",
    "            ref.append(valor_tweet) \n",
    "            sentimentList.append(sentimiento)\n",
    "            if sentimiento > 8 and sentimiento < 13:\n",
    "                testVal.append(2)\n",
    "            elif(sentimiento <= 8): \n",
    "                testVal.append(-1)\n",
    "            else:\n",
    "                testVal.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardamos los resultados de la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file1 = open(filename_1 + '.txt',\"w\") \n",
    "\n",
    "accuracyDict = {}\n",
    "  \n",
    "# \\n is placed to indicate EOL (End of Line) \n",
    "file1.write(\"Resultados \"+ filename_1 + ' \\n\\n') \n",
    "\n",
    "writer = pd.ExcelWriter(path+directory+'clasification_report_'+file_1+'.xlsx', engine='xlsxwriter')\n",
    "writerCM = pd.ExcelWriter(path+directory+'confusion_matrix_'+file_1+'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#     print(\"***** METRICAS CLASIFICADOR \"+nombre+\" *****\")\n",
    "file1.write(\"***** METRICAS DICCIONARIO: SentiCon *****\\n\\n\") \n",
    "#     print(\"Exactitud (accuracy) \" +  str(accuracy(ref, test)*100))\n",
    "file1.write(\"Exactitud (accuracy) \" +  str(accuracy(ref, testVal)*100)+\"\\n\") \n",
    "reference_set = set(ref)\n",
    "test_set = set(testVal)\n",
    "#     print(\"Precision \" +str(precision(reference_set, test_set)))\n",
    "file1.write(\"Precision \" +str(precision(reference_set, test_set))+\"\\n\")\n",
    "#     print(\"Recall \" +str(recall(reference_set, test_set)))\n",
    "file1.write(\"Recall \" +str(recall(reference_set, test_set))+\"\\n\")\n",
    "#     print(\"F measure \" + str(f_measure(reference_set, test_set)))\n",
    "file1.write(\"F measure \" + str(f_measure(reference_set, test_set))+\"\\n\\n\")\n",
    "file1.write(\"Matriz de confusión \\n\")\n",
    "#     print(ConfusionMatrix(ref, test))   \n",
    "file1.write(str(ConfusionMatrix(ref, testVal)))\n",
    "file1.write('\\n')\n",
    "file1.write(\"Reporte de clasificación\\n\")\n",
    "#     print(metrics.classification_report(ref,test,digits=3))\n",
    "file1.write(metrics.classification_report(ref,testVal,digits=3)+'\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "data = {'y_Predicted': testVal,\n",
    "    'y_Actual':   ref\n",
    "    }\n",
    "\n",
    "df_cm = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_cm['y_Actual'], df_cm['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "confusion_matrix.to_excel(writerCM,sheet_name=nombre) \n",
    "\n",
    "report = metrics.classification_report(ref, testVal, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_excel(writer,sheet_name=nombre)\n",
    "\n",
    "\n",
    "file1.write(\"****************************************\\n\\n\\n\\n\")\n",
    "\n",
    "accuracyDict[nombre] = str(accuracy(ref, testVal)*100)\n",
    "\n",
    "df_ac = pd.DataFrame([accuracyDict])   \n",
    "df_ac = df_ac.T\n",
    "df_ac.to_excel(path+directory+'accuracy_'+file_1+'.xlsx')\n",
    "\n",
    "file1.close() #to change file access modes \n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicamos stemming a la clasificación con diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función que aplica el algoritmo de stemming a un texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizacion(row):\n",
    "    result = stemmer.stem(row['Palabra'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentimiento</th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Categoría</th>\n",
       "      <th>Posicion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>acert</td>\n",
       "      <td>0.708</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>admir</td>\n",
       "      <td>0.906</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>admir</td>\n",
       "      <td>0.450</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>admir</td>\n",
       "      <td>0.750</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>afect</td>\n",
       "      <td>0.375</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pos</td>\n",
       "      <td>afect</td>\n",
       "      <td>0.321</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pos</td>\n",
       "      <td>afectu</td>\n",
       "      <td>0.563</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pos</td>\n",
       "      <td>aficion</td>\n",
       "      <td>0.500</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pos</td>\n",
       "      <td>afortun</td>\n",
       "      <td>0.813</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pos</td>\n",
       "      <td>agrad</td>\n",
       "      <td>0.750</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pos</td>\n",
       "      <td>agradec</td>\n",
       "      <td>0.500</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pos</td>\n",
       "      <td>alegr</td>\n",
       "      <td>0.458</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pos</td>\n",
       "      <td>alegr</td>\n",
       "      <td>0.661</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pos</td>\n",
       "      <td>alent</td>\n",
       "      <td>0.594</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pos</td>\n",
       "      <td>alent</td>\n",
       "      <td>0.275</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pos</td>\n",
       "      <td>amabl</td>\n",
       "      <td>0.688</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pos</td>\n",
       "      <td>amar</td>\n",
       "      <td>1.000</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pos</td>\n",
       "      <td>amist</td>\n",
       "      <td>0.708</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pos</td>\n",
       "      <td>amor</td>\n",
       "      <td>0.278</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pos</td>\n",
       "      <td>anim</td>\n",
       "      <td>0.494</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pos</td>\n",
       "      <td>anim</td>\n",
       "      <td>0.289</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pos</td>\n",
       "      <td>apac</td>\n",
       "      <td>0.313</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pos</td>\n",
       "      <td>apasion</td>\n",
       "      <td>0.469</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pos</td>\n",
       "      <td>apeg</td>\n",
       "      <td>0.375</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pos</td>\n",
       "      <td>apreci</td>\n",
       "      <td>0.344</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pos</td>\n",
       "      <td>aprob</td>\n",
       "      <td>0.375</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pos</td>\n",
       "      <td>ardient</td>\n",
       "      <td>0.313</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pos</td>\n",
       "      <td>atract</td>\n",
       "      <td>0.844</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pos</td>\n",
       "      <td>benefici</td>\n",
       "      <td>0.396</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pos</td>\n",
       "      <td>benevolent</td>\n",
       "      <td>0.438</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11488</th>\n",
       "      <td>neg</td>\n",
       "      <td>versicul</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11489</th>\n",
       "      <td>neg</td>\n",
       "      <td>vesani</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11490</th>\n",
       "      <td>neg</td>\n",
       "      <td>vesan</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11492</th>\n",
       "      <td>neg</td>\n",
       "      <td>vestidur</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11493</th>\n",
       "      <td>neg</td>\n",
       "      <td>vestigial</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>neg</td>\n",
       "      <td>vicealmir</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>neg</td>\n",
       "      <td>viej</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11503</th>\n",
       "      <td>neg</td>\n",
       "      <td>villan</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>neg</td>\n",
       "      <td>vislumbr</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11508</th>\n",
       "      <td>neg</td>\n",
       "      <td>viud</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11509</th>\n",
       "      <td>neg</td>\n",
       "      <td>viud</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11513</th>\n",
       "      <td>neg</td>\n",
       "      <td>vulcanit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11514</th>\n",
       "      <td>neg</td>\n",
       "      <td>vulner</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>neg</td>\n",
       "      <td>victim</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>neg</td>\n",
       "      <td>whodunit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>neg</td>\n",
       "      <td>xenofobi</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>neg</td>\n",
       "      <td>xenolit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11522</th>\n",
       "      <td>neg</td>\n",
       "      <td>zangarrian</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>neg</td>\n",
       "      <td>zidovudin</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11524</th>\n",
       "      <td>neg</td>\n",
       "      <td>ziti</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11525</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoo</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11526</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoofobi</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11527</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoolog</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11528</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoonosis</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11529</th>\n",
       "      <td>neg</td>\n",
       "      <td>zorr</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11530</th>\n",
       "      <td>neg</td>\n",
       "      <td>zurc</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>neg</td>\n",
       "      <td>azo</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>neg</td>\n",
       "      <td>obit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>neg</td>\n",
       "      <td>osmosis</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>neg</td>\n",
       "      <td>otic</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentimiento     Palabra  Categoría Posicion\n",
       "0             pos       acert      0.708        a\n",
       "1             pos       admir      0.906        a\n",
       "2             pos       admir      0.450        n\n",
       "3             pos       admir      0.750        v\n",
       "4             pos       afect      0.375        a\n",
       "5             pos       afect      0.321        n\n",
       "6             pos      afectu      0.563        a\n",
       "7             pos     aficion      0.500        n\n",
       "8             pos     afortun      0.813        a\n",
       "9             pos       agrad      0.750        a\n",
       "10            pos     agradec      0.500        a\n",
       "11            pos       alegr      0.458        v\n",
       "12            pos       alegr      0.661        a\n",
       "13            pos       alent      0.594        a\n",
       "14            pos       alent      0.275        v\n",
       "15            pos       amabl      0.688        a\n",
       "16            pos        amar      1.000        v\n",
       "17            pos       amist      0.708        a\n",
       "18            pos        amor      0.278        n\n",
       "19            pos        anim      0.494        a\n",
       "20            pos        anim      0.289        v\n",
       "21            pos        apac      0.313        a\n",
       "22            pos     apasion      0.469        a\n",
       "23            pos        apeg      0.375        n\n",
       "24            pos      apreci      0.344        n\n",
       "25            pos       aprob      0.375        v\n",
       "26            pos     ardient      0.313        a\n",
       "27            pos      atract      0.844        a\n",
       "28            pos    benefici      0.396        n\n",
       "29            pos  benevolent      0.438        n\n",
       "...           ...         ...        ...      ...\n",
       "11488         neg    versicul     -0.250        n\n",
       "11489         neg      vesani     -0.250        n\n",
       "11490         neg       vesan     -0.250        n\n",
       "11492         neg    vestidur     -0.250        n\n",
       "11493         neg   vestigial     -0.250        a\n",
       "11497         neg   vicealmir     -0.250        n\n",
       "11498         neg        viej     -0.250        n\n",
       "11503         neg      villan     -0.250        n\n",
       "11507         neg    vislumbr     -0.250        n\n",
       "11508         neg        viud     -0.250        n\n",
       "11509         neg        viud     -0.250        n\n",
       "11513         neg    vulcanit     -0.250        n\n",
       "11514         neg      vulner     -0.250        n\n",
       "11518         neg      victim     -0.250        n\n",
       "11519         neg    whodunit     -0.250        n\n",
       "11520         neg    xenofobi     -0.250        n\n",
       "11521         neg     xenolit     -0.250        n\n",
       "11522         neg  zangarrian     -0.250        n\n",
       "11523         neg   zidovudin     -0.250        n\n",
       "11524         neg        ziti     -0.250        n\n",
       "11525         neg         zoo     -0.250        n\n",
       "11526         neg     zoofobi     -0.250        n\n",
       "11527         neg      zoolog     -0.250        n\n",
       "11528         neg    zoonosis     -0.250        n\n",
       "11529         neg        zorr     -0.250        n\n",
       "11530         neg        zurc     -0.250        n\n",
       "11537         neg         azo     -0.250        n\n",
       "11538         neg        obit     -0.250        n\n",
       "11540         neg     osmosis     -0.250        n\n",
       "11541         neg        otic     -0.250        a\n",
       "\n",
       "[8760 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_label= dict_label_stem\n",
    "dict_label['Palabra'] = dict_label.apply(lematizacion, axis=1)\n",
    "dict_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elimnamos las palabras duplicadas que puedan estar en el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentimiento</th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Categoría</th>\n",
       "      <th>Posicion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>acert</td>\n",
       "      <td>0.708</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>admir</td>\n",
       "      <td>0.906</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>afect</td>\n",
       "      <td>0.375</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pos</td>\n",
       "      <td>afectu</td>\n",
       "      <td>0.563</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pos</td>\n",
       "      <td>aficion</td>\n",
       "      <td>0.500</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pos</td>\n",
       "      <td>afortun</td>\n",
       "      <td>0.813</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pos</td>\n",
       "      <td>agrad</td>\n",
       "      <td>0.750</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pos</td>\n",
       "      <td>agradec</td>\n",
       "      <td>0.500</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pos</td>\n",
       "      <td>alegr</td>\n",
       "      <td>0.458</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pos</td>\n",
       "      <td>alent</td>\n",
       "      <td>0.594</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pos</td>\n",
       "      <td>amabl</td>\n",
       "      <td>0.688</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pos</td>\n",
       "      <td>amar</td>\n",
       "      <td>1.000</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pos</td>\n",
       "      <td>amist</td>\n",
       "      <td>0.708</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pos</td>\n",
       "      <td>amor</td>\n",
       "      <td>0.278</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pos</td>\n",
       "      <td>anim</td>\n",
       "      <td>0.494</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pos</td>\n",
       "      <td>apac</td>\n",
       "      <td>0.313</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pos</td>\n",
       "      <td>apasion</td>\n",
       "      <td>0.469</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pos</td>\n",
       "      <td>apeg</td>\n",
       "      <td>0.375</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pos</td>\n",
       "      <td>apreci</td>\n",
       "      <td>0.344</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pos</td>\n",
       "      <td>aprob</td>\n",
       "      <td>0.375</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pos</td>\n",
       "      <td>ardient</td>\n",
       "      <td>0.313</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pos</td>\n",
       "      <td>atract</td>\n",
       "      <td>0.844</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pos</td>\n",
       "      <td>benefici</td>\n",
       "      <td>0.396</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pos</td>\n",
       "      <td>benevolent</td>\n",
       "      <td>0.438</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pos</td>\n",
       "      <td>bien</td>\n",
       "      <td>0.833</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>pos</td>\n",
       "      <td>bienest</td>\n",
       "      <td>0.531</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pos</td>\n",
       "      <td>bond</td>\n",
       "      <td>0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pos</td>\n",
       "      <td>bonit</td>\n",
       "      <td>0.750</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pos</td>\n",
       "      <td>buen</td>\n",
       "      <td>0.734</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>pos</td>\n",
       "      <td>cariñ</td>\n",
       "      <td>0.438</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11484</th>\n",
       "      <td>neg</td>\n",
       "      <td>ventos</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11485</th>\n",
       "      <td>neg</td>\n",
       "      <td>ventriloqui</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11486</th>\n",
       "      <td>neg</td>\n",
       "      <td>veratrum</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11487</th>\n",
       "      <td>neg</td>\n",
       "      <td>verdug</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11488</th>\n",
       "      <td>neg</td>\n",
       "      <td>versicul</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11489</th>\n",
       "      <td>neg</td>\n",
       "      <td>vesani</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11490</th>\n",
       "      <td>neg</td>\n",
       "      <td>vesan</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11492</th>\n",
       "      <td>neg</td>\n",
       "      <td>vestidur</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11493</th>\n",
       "      <td>neg</td>\n",
       "      <td>vestigial</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>neg</td>\n",
       "      <td>vicealmir</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>neg</td>\n",
       "      <td>viej</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>neg</td>\n",
       "      <td>vislumbr</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11508</th>\n",
       "      <td>neg</td>\n",
       "      <td>viud</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11513</th>\n",
       "      <td>neg</td>\n",
       "      <td>vulcanit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>neg</td>\n",
       "      <td>victim</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>neg</td>\n",
       "      <td>whodunit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>neg</td>\n",
       "      <td>xenofobi</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>neg</td>\n",
       "      <td>xenolit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11522</th>\n",
       "      <td>neg</td>\n",
       "      <td>zangarrian</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>neg</td>\n",
       "      <td>zidovudin</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11524</th>\n",
       "      <td>neg</td>\n",
       "      <td>ziti</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11525</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoo</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11526</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoofobi</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11527</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoolog</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11528</th>\n",
       "      <td>neg</td>\n",
       "      <td>zoonosis</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11529</th>\n",
       "      <td>neg</td>\n",
       "      <td>zorr</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11530</th>\n",
       "      <td>neg</td>\n",
       "      <td>zurc</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>neg</td>\n",
       "      <td>azo</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>neg</td>\n",
       "      <td>obit</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>neg</td>\n",
       "      <td>otic</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7199 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentimiento      Palabra  Categoría Posicion\n",
       "0             pos        acert      0.708        a\n",
       "1             pos        admir      0.906        a\n",
       "4             pos        afect      0.375        a\n",
       "6             pos       afectu      0.563        a\n",
       "7             pos      aficion      0.500        n\n",
       "8             pos      afortun      0.813        a\n",
       "9             pos        agrad      0.750        a\n",
       "10            pos      agradec      0.500        a\n",
       "11            pos        alegr      0.458        v\n",
       "13            pos        alent      0.594        a\n",
       "15            pos        amabl      0.688        a\n",
       "16            pos         amar      1.000        v\n",
       "17            pos        amist      0.708        a\n",
       "18            pos         amor      0.278        n\n",
       "19            pos         anim      0.494        a\n",
       "21            pos         apac      0.313        a\n",
       "22            pos      apasion      0.469        a\n",
       "23            pos         apeg      0.375        n\n",
       "24            pos       apreci      0.344        n\n",
       "25            pos        aprob      0.375        v\n",
       "26            pos      ardient      0.313        a\n",
       "27            pos       atract      0.844        a\n",
       "28            pos     benefici      0.396        n\n",
       "29            pos   benevolent      0.438        n\n",
       "31            pos         bien      0.833        a\n",
       "33            pos      bienest      0.531        n\n",
       "34            pos         bond      0.250        n\n",
       "35            pos        bonit      0.750        a\n",
       "36            pos         buen      0.734        a\n",
       "38            pos        cariñ      0.438        a\n",
       "...           ...          ...        ...      ...\n",
       "11484         neg       ventos     -0.250        n\n",
       "11485         neg  ventriloqui     -0.250        n\n",
       "11486         neg     veratrum     -0.250        n\n",
       "11487         neg       verdug     -0.250        n\n",
       "11488         neg     versicul     -0.250        n\n",
       "11489         neg       vesani     -0.250        n\n",
       "11490         neg        vesan     -0.250        n\n",
       "11492         neg     vestidur     -0.250        n\n",
       "11493         neg    vestigial     -0.250        a\n",
       "11497         neg    vicealmir     -0.250        n\n",
       "11498         neg         viej     -0.250        n\n",
       "11507         neg     vislumbr     -0.250        n\n",
       "11508         neg         viud     -0.250        n\n",
       "11513         neg     vulcanit     -0.250        n\n",
       "11518         neg       victim     -0.250        n\n",
       "11519         neg     whodunit     -0.250        n\n",
       "11520         neg     xenofobi     -0.250        n\n",
       "11521         neg      xenolit     -0.250        n\n",
       "11522         neg   zangarrian     -0.250        n\n",
       "11523         neg    zidovudin     -0.250        n\n",
       "11524         neg         ziti     -0.250        n\n",
       "11525         neg          zoo     -0.250        n\n",
       "11526         neg      zoofobi     -0.250        n\n",
       "11527         neg       zoolog     -0.250        n\n",
       "11528         neg     zoonosis     -0.250        n\n",
       "11529         neg         zorr     -0.250        n\n",
       "11530         neg         zurc     -0.250        n\n",
       "11537         neg          azo     -0.250        n\n",
       "11538         neg         obit     -0.250        n\n",
       "11541         neg         otic     -0.250        a\n",
       "\n",
       "[7199 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_label = dict_label.drop_duplicates(subset=['Palabra'])\n",
    "dict_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esta función determina la polaridad de un tweet en funcion del diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CON STEMMING\n",
    "def analisis_sentimiento(text):\n",
    "    tienePalabra = False\n",
    "    valor = 999999\n",
    "    filtered_words = [word for word in text.split() if word not in stopwords.words('spanish')]\n",
    "    for word in filtered_words:\n",
    "        if dict_label.loc[dict_label['Palabra'].isin([stemmer.stem(word)])]['Categoría'].any(): \n",
    "            if not tienePalabra:\n",
    "                tienePalabra = True\n",
    "                valor = 0\n",
    "            try:                \n",
    "                valor = valor + float(dict_label.loc[dict_label['Palabra'].isin([stemmer.stem(word)])]['Categoría'])\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(dict_label.loc[dict_label['Palabra'].isin([word])]['Categoría'])               \n",
    "    \n",
    "    return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se calcula la polaridad del conjunto de tweets de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correcto = 0\n",
    "max = 0\n",
    "min = 0\n",
    "ref = []\n",
    "testVal= []\n",
    "\n",
    "if(clasificacion == 2):\n",
    "    #BINARIO\n",
    "    for id, tweet in test.iterrows(): \n",
    "        sentimiento = analisis_sentimiento(str(tweet.text))\n",
    "        if(sentimiento != 999999):\n",
    "            if(sentimiento > max):\n",
    "                max = sentimiento\n",
    "            if(sentimiento < min):\n",
    "                min = sentimiento  \n",
    "            ref.append(1 if int(tweet.sentiment) == 1 else -1)      \n",
    "            if(sentimiento <= 1): \n",
    "                testVal.append(-1)\n",
    "            else:\n",
    "                testVal.append(1)\n",
    "        \n",
    "else:\n",
    "    #TERNARIO\n",
    "    for id, tweet in test.iterrows():    \n",
    "        sentimiento = analisis_sentimiento(str(tweet.text))\n",
    "        if (int(tweet.sentiment) == 0):\n",
    "            valor_tweet = -1\n",
    "        else: \n",
    "            valor_tweet = int(tweet.sentiment)\n",
    "        if(sentimiento != 999999):\n",
    "            if(sentimiento > max):\n",
    "                max = sentimiento\n",
    "            if(sentimiento < min):\n",
    "                min = sentimiento\n",
    "            ref.append(valor_tweet)  \n",
    "            if sentimiento > 0.5 and sentimiento < 1.5:\n",
    "                testVal.append(2)\n",
    "            elif(sentimiento <= 0): \n",
    "                testVal.append(-1)\n",
    "            else:\n",
    "                testVal.append(1)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardamos los resultados de la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file1 = open(filename_2 + '.txt',\"w\") \n",
    "\n",
    "accuracyDict = {}\n",
    "  \n",
    "# \\n is placed to indicate EOL (End of Line) \n",
    "file1.write(\"Resultados \"+ filename_2 + ' \\n\\n') \n",
    "\n",
    "writer = pd.ExcelWriter(path+directory+'clasification_report_'+file_2+'_STEMMING.xlsx', engine='xlsxwriter')\n",
    "writerCM = pd.ExcelWriter(path+directory+'confusion_matrix_'+file_2+'_STEMMING.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#     print(\"***** METRICAS CLASIFICADOR \"+nombre+\" *****\")\n",
    "file1.write(\"***** METRICAS DICCIONARIO: SentiCon *****\\n\\n\") \n",
    "#     print(\"Exactitud (accuracy) \" +  str(accuracy(ref, test)*100))\n",
    "file1.write(\"Exactitud (accuracy) \" +  str(accuracy(ref, testVal)*100)+\"\\n\") \n",
    "reference_set = set(ref)\n",
    "test_set = set(testVal)\n",
    "#     print(\"Precision \" +str(precision(reference_set, test_set)))\n",
    "file1.write(\"Precision \" +str(precision(reference_set, test_set))+\"\\n\")\n",
    "#     print(\"Recall \" +str(recall(reference_set, test_set)))\n",
    "file1.write(\"Recall \" +str(recall(reference_set, test_set))+\"\\n\")\n",
    "#     print(\"F measure \" + str(f_measure(reference_set, test_set)))\n",
    "file1.write(\"F measure \" + str(f_measure(reference_set, test_set))+\"\\n\\n\")\n",
    "file1.write(\"Matriz de confusión \\n\")\n",
    "#     print(ConfusionMatrix(ref, test))   \n",
    "file1.write(str(ConfusionMatrix(ref, testVal)))\n",
    "file1.write('\\n')\n",
    "file1.write(\"Reporte de clasificación\\n\")\n",
    "#     print(metrics.classification_report(ref,test,digits=3))\n",
    "file1.write(metrics.classification_report(ref,testVal,digits=3)+'\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "data = {'y_Predicted': testVal,\n",
    "    'y_Actual':   ref\n",
    "    }\n",
    "\n",
    "df_cm = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_cm['y_Actual'], df_cm['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "confusion_matrix.to_excel(writerCM,sheet_name=nombre) \n",
    "\n",
    "report = metrics.classification_report(ref, testVal, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_excel(writer,sheet_name=nombre)\n",
    "\n",
    "\n",
    "file1.write(\"****************************************\\n\\n\\n\\n\")\n",
    "\n",
    "accuracyDict[nombre] = str(accuracy(ref, testVal)*100)\n",
    "\n",
    "df_ac = pd.DataFrame([accuracyDict])   \n",
    "df_ac = df_ac.T\n",
    "df_ac.to_excel(path+directory+'accuracy_'+file_2+'_STEMMING.xlsx')\n",
    "\n",
    "file1.close() #to change file access modes \n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
